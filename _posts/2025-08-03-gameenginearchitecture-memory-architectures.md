---
title: "[GameEngineArchitecture] Memory Architectures"
date: 2025-08-03
toc: true
categories:
  - "Tistory"
tags:
  - "Cache"
  - "CacheHit"
  - "CacheMiss"
  - "MMU"
  - "virtual"
  - "메모리"
  - "캐쉬"
  - "캐시"
  - "캐시미스"
  - "캐시힛"
---

간단한 폰 노이만 컴퓨터 구조에서는, 메모리가 CPU에 의해 동일하게 접근 가능한 동일한 블록으로 취급된다.

하지만 실제로 컴퓨터의 메모리는 그렇게 단순하게 설계되는 경우는 거의 없다.

오늘날의 개인용 컴퓨터나 게임 콘솔에서 흔히 볼 수 있는 몇 가지 메모리 아키텍처들을 살펴보고,  
그들이 그런 식으로 설계된 핵심적인 이유들을 알아보자

### 

### **Memory Mapping**

n비트 주소 버스는 CPU가 이론적으로 2^n바이트 크기의 주소 공간에 접근할 수 있게 해준다.

ROM/RAM은 항상 연속적인 메모리 셀 블록으로 주소가 지정된다.

물리적인 메모리 장치가 컴퓨터의 주소 공간 내 특정 주소 범위에 할당될 때, 우리는 그 주소 범위가 해당 메모리 장치에 **매핑되었다(mapped)** 라고 말한다.

물론 컴퓨터는 주소 버스가 이론적으로 접근할 수 있는 만큼의 메모리를 실제로 장착하고 있지 않을 수도 있다.  
64비트 주소 버스는 **16 엑사바이트(EiB)** 의 메모리에 접근할 수 있지만, 그런 주소 공간을 완전히 채우는 일은 거의 없을 것이다.  
(참고로 HP의 프로토타입 슈퍼컴퓨터인 The Machine조차 160 테라바이트(TiB)에 불과하여, 해당 한계에 한참 못 미친다.)

따라서 컴퓨터의 주소 공간 일부가 **할당되지 않은 상태로 남아 있는 경우**가 일반적이다.

### 

### **Memory-Mapped I/O**

주소 범위는 반드시 모두 메모리 장치에 매핑될 필요는 없다. 조이패드, NIC와 같은 **다른 주변 장치**에 매핑될 수도 있다.

이러한 방식은 **메모리 매핑 I/O(memory-mapped I/O)** 라고 부른다.

CPU가 주변 장치에 대한 I/O 작업을 수행할 때 마치 **일반적인 RAM에 접근하듯이** **주소에 대한 읽기 또는 쓰기 연산**을 통해 작업을 수행할 수 있게 한다.

내부적으로는, CPU가 **비메모리 장치에 매핑된 주소 범위**를 읽거나 쓴다는 사실을 **특수한 회로**가 감지하고, 해당 읽기 또는 쓰기 요청을 **대상 장치에 대한 I/O 작업**으로 변환한다.

또 다른 방식으로, **port-mapped I/O가 있다.**

**CPU는 포트(port) 라고 불리는 특수 레지스터를 통해 비메모리 장치와 통신하는 방법이다.**  
이 경우, CPU가 포트 레지스터로부터 데이터를 읽거나 쓰도록 요청하면, **하드웨어가 해당 요청을 대상 장치에 대한 I/O 작업으로 변환한다.**

### 

### **Video RAM (VRAM)**

래스터 기반(raster-based)의 디스플레이 장치는,  
**화면의 각 픽셀의 밝기나 색상을 결정하기 위해** **하드웨어적으로 고정된 물리적 메모리 주소 범위**를 읽는다.   
**비디오 컨트롤러**에 의해 사용되도록 할당된 메모리 주소 범위를 **비디오 RAM (VRAM)** 이라고 부른다.

=> VRAM(Video RAM)은 GPU가 디스플레이에 출력할 데이터를 저장하는 전용 메모리 공간이다.

초기 컴퓨터들은 VRAM이 마더보드 상의 일반 RAM과 같이 존재했지만,

현대에는 GPU에 VARM이 탑재되어서 나오기 때문에

CPU와 GPU는 VRAM을 공유하지 않고, 서로 다른 주소 공간을 가지게 된다.

이런 이유로 CPU가 데이터를 GPU의 VRAM에 쓰려면 PCIe를 통해 전송해야 하고, 이게 곧 병목의 원인이 된다.

그래픽 API (Dx, openGL, Vulkan 등) 이 복잡한 이유가 CPU-GPU간 통신 때문이다.

VRAM 접근, GPU에 맞는 format, 비동이성 등등

## 

## **Virtual Memory (가상 메모리)**

**대부분의 현대 CPU와 운영체제는 메모리 리매핑 기능, 즉 가상 메모리 시스템을 지원한다.**

이 시스템에서는 **프로그램이 사용하는 주소가 컴퓨터에 실제로 설치된 메모리와 직접 연결되지 않도록 도와준다.**

대신, 프로그램이 주소에 대해 읽기 또는 쓰기 요청을 할 때, **CPU가 운영체제가 유지하는 테이블을 참고하여 해당 주소를 다시 매핑**한다.

1. 리매핑된 주소는 실제 메모리(물리적 주소)를 참조할 수도 있고,

2. 하드디스크에 저장된 데이터 블록을 참조할 수도 있으며,

3. 전혀 어떤 물리적 저장소에도 매핑되지 않았을 수도 있다.

이런 가상 메모리 시스템에서 프로그램이 사용하는 주소는 **가상 주소(Virtual Address)** 라고 하며,

메모리 컨트롤러가 실제 RAM이나 ROM을 접근할 때 사용하는 주소는 **물리 주소(Physical Address)** 라고 한다.

**장점**

실제 컴퓨터에 장착된 메모리보다 **더 많은 메모리를 사용하는 것처럼 프로그램이 작동할 수 있다**

운영체제의 **안정성과 보안성을 향상**시킬 수 있다

=> 각 프로그램은 고유한 가상 메모리 공간을 가지기 때문에, OS의 메모리를 덮는 것을 방지할 수 있다.

## 

## **Virtual Memory Pages (가상 메모리 페이지)**

가상 주소-물리 주소의 리매핑이 어떻게 이뤄지는지 이해하려면, 먼저 전체 주소 공간을 **페이지(page)** 라는 개념으로 나누어야 한다.

주소 공간은 2^n 바이트 크기의 메모리 셀로 이루어져 있고, 이들을 **연속적이고 균등한 크기의 페이지 단위로 나눈다**

페이지 크기는 OS마다 다르지만, 항상 **2의 제곱수**이다

(일반적으로는 **4KiB 또는 8KiB**)

예를 들어,

주소 공간이 32비트라면 전제 주소 공간은 2^32 (약 4GB)일 것이고

페이지 크기가 4KiB라면 (2^12)

이 공간은 1,048,576개 (2^(32 - 12) 개) 의 페이지로 나뉘어진다.

![](https://blog.kakaocdn.net/dna/bpeyD4/btsPEw2Sna6/AAAAAAAAAAAAAAAAAAAAAIkrbCRINMSStd-_S7pZgbh2AMcm1a9NU2uMob8pY7hn/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1772290799&allow_ip=&allow_referer=&signature=RPniXA1r2eUM2lQ%2BoVpW2T0gS3o%3D)

, 가상 주소와 물리 주소 간의 매핑은 페이지 단위 로 이루어진다.

가상 주소와 물리 주소 간의 매핑은 페이지 단위로 이루어진다.

(PageIndex가 사용하는 공간: from Address ~ To Address)

## 

## **Virtual to Physical Address Translation(가상 주소-물리 주소 변환)**

CPU가 **메모리 읽기 또는 쓰기 작업을 감지**할 때마다, 해당 주소는 두 부분으로 분리된다.

**페이지 인덱스 (Page Index)**

**페이지 내부 오프셋 (Offset)**  
=>이 offset은 바이트 단위이다.

만약 페이지 크기가 4KiB인 경우를 보자

**페이지의 크기가 4KiB = 2^12바이트**이므로, 위치를 지정하려면 12비트가 필요할 것이다.

그러니 주소의 **하위 12비트로 offset을 지정하고**, **상위 20비트로는 page index를 지정한다.**

(주소 공간이 32비트니까, 12비트로 offset을 정하고 남은 20비트로 page index를 정한다.)

![](https://blog.kakaocdn.net/dna/bOLTLr/btsPEPgOkvP/AAAAAAAAAAAAAAAAAAAAAFoPn5vn1rPp6hI39F9YjcjOMeM2KLaTzRuAktMFAnwq/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1772290799&allow_ip=&allow_referer=&signature=ggbxYD%2BhF9opj0Yrn%2BCX%2Bu7yS5g%3D)

### 

### Page Fault (페이지 폴트)

페이지 테이블에 해당 페이지가 **매핑되지 않았을 경우**, 두 가지 상황이 발생할 수 있다.

1. 페이지가 **아예 할당되지 않았거나**

2. 페이지가 **디스크로 스왑되어 메모리에 없는 상태일 때**

이 경우 **MMU가 인터럽트(page fault interrupt)를 발생시킨다.**

운영체제는 **메모리 요청을 즉시 처리할 수 없다는 신호를 받게된다.**

(MMU(Memory Management Unit):Virtual Memory를 Physcial Memory로 변환해주는 장치)

## 

## **Handling Page Faults**

**1.할당되지 않은 페이지에 접근한 경우**

운영체제는 보통 해당 프로그램을 **크래시시키고, 코어 덤프를 생성**한다.  
=>메모리 접근 오류로 프로그램 종료됨

(코어 덤프: 프로그램이 비정상적으로 종료될 때, 그 시점의 메모리 상태를 통째로 저장한 파일이다. 이를 분석해서 프로그램이 왜 비정상적인 종료가 되었는지 분석할 수 있다.)

**2. 스왑된(out) 페이지에 접근한 경우**

운영체제는 현재 실행 중인 프로그램을 **일시 중단**하고, **디스크에 저장된 해당 페이지를 RAM으로 불러옴**

그 이후  RAM의 사용가능한 물리 페이지에 페이지를 로드한다.

그리고 다시 Vitrual Address를 Physcial Address로 바꾸고, 프로그램을 실행시킨다.

이 전체 과정은 프로그램 입장에서 seamless(매끄럽다 자연스럽다)하기에   
즉, **페이지가 이미 메모리에 있었는지 디스크에서 불러온 것인지를 알 수 없음**

운영체제는 보통 **메모리 부담이 높을 때만 페이지를 디스크로 스왑**한다.

또한 자주 쓰지 않는 메모리 페이지(= LRU, Least Recently Used)를 스왑하려고 한다.

그래야 프로그램이 메모리와 디스크 사이에서 **불필요한 교체(thrashing)**하지 않도록 방지할 수 있다.

![](https://blog.kakaocdn.net/dna/XyWGl/btsPDEG7hbX/AAAAAAAAAAAAAAAAAAAAAKiaCXrAcY30NDd3L7tM6AcmHHKwiwvLTa8kWVVJNa-o/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1772290799&allow_ip=&allow_referer=&signature=jem8qaveoJg6u76O9zivYdcs3IA%3D)

### 

### **TLB, Translation Lookaside Buffer**

페이지 크기는 일반적으로 전체 주소 지정 가능한 메모리 크기에 비해 작기 때문에 (보통 4KiB 또는 8KiB), **페이지 테이블은 매우 커질 수 있다.**

**만약 프로그램이 메모리에 접근할 때마다 전체 페이지 테이블을 매번 검색해야 한다면, 물리 주소를 찾는 데 시간이 너무 오래 걸릴 것이다.**

=> 이 문제를 해결하기 위해 **캐싱 메커니즘**이 사용된다.

이는 대부분의 프로그램이 메모리 주소를 **완전히 무작위로 접근하지 않고**, **일정한 소수의 페이지 내에서 반복해서 접근**한다는 가정에 기반한다.

CPU 내부의 MMU(Memory Management Unit)에는 TLB(변환 캐시)라 불리는 **작은 테이블**이 유지되며,

최근에 사용된 가상 주소-물리 주소 매핑 정보들이 이곳에 캐시된다. 이 캐시는 MMU 가까이에 위치하므로 **접근 속도가 매우 빠르다**.

TLB는 일반적인 메모리 캐시 계층과 유사하게 작동하지만, **오직 페이지 테이블 항목만 캐시한다는 점에서 다르다**.

### 

### **latency(지연) 감소를 위한 메모리 아키텍처**

데이터를 메모리 장치로부터 **얼마나 빠르게 접근할 수 있는가**는 매우 중요한 특성이다.

이때 사용하는 개념이 **메모리 접근 지연 시간(latency**이며, 이는 CPU가 데이터를 요청한 순간부터 실제 데이터를 수신하는 순간까지의 시간을 의미한다.

이 지연 시간은 주로 다음 세 가지 요소에 의해 결정된다.

**1. 개별 메모리 셀을 구현한 기술**

SRAM (정적 RAM)은 DRAM (동적 RAM)보다 일반적으로 접근 지연 시간이 낮다. (빠르다)  
SRAM은 보다 복잡한 설계를 사용하며, **비트당 더 많은 트랜지스터**가 필요하기 때문에  
**DRAM보다 비싸고, 실리콘 칩 위의 면적도 더 많이 차지**한다.

**2. 읽기/쓰기 포트 수**

단순한 메모리 셀은 **단일 포트를 가지며**, 특정 시점에 **한 번의 읽기 또는 쓰기**만 가능하다.

**멀티포트 메모리**는 **여러 번의 읽기/쓰기 연산을 동시에** 수행할 수 있어서  
**여러 코어나 CPU 내부 구성 요소가 동시에 메모리에 접근할 때** 생기는 지연 시간을 줄일 수 있다.

물론 멀티포트 메모리는 더 많은 트랜지스터를 요구하며, **비용과 면적이 증가**한다.

**3. CPU와 메모리 셀 간의 물리적 거리**

또한 **CPU와 메모리 뱅크 간의 물리적 거리**도 지연 시간에 영향을 준다.  
전자 신호는 컴퓨터 내부에서 일정한 속도로 이동하기 때문에,  
**거리가 멀수록 지연 시간은 더 길어질 수 있다.**

이론상 전자 신호는 **전자기파**로 구성되어 있어 **광속에 가깝게** 이동하지만,  
실제로는 다양한 **스위칭 회로 및 논리 회로**를 거치기 때문에  
추가적인 지연이 발생한다.

SRAM vs DRAM에 대한 좋은 영상입니다.

<https://www.youtube.com/watch?v=r787m_IaR1I>



### 

### **The Memory Gap**

컴퓨터 초창기에는 **메모리 접근 지연 시간**과 **명령어 실행 시간**이 거의 비슷했다.

하지만 이후 수십 년 동안, CPU의 **클럭 속도**와 **명령어 처리 속도**는  
**메모리 접근 속도보다 훨씬 빠른 속도로 발전**해왔다.

오늘날에는 한 번의 명령어 실행(레지스터 기반 연산)은 여전히  
**1~10 사이클** 안에 끝나지만,  
**주 메모리 접근**은 **500 사이클 이상**이 걸릴 수 있다!

이러한 CPU와 메모리 간의 성능 차이를 memory gap라고 부른다.

![](https://blog.kakaocdn.net/dna/m7c4u/btsPFiXdmWI/AAAAAAAAAAAAAAAAAAAAALSpR1jP131LvVo0ELhskWbs1aBa9UtPaExaro-T_oag/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1772290799&allow_ip=&allow_referer=&signature=FUPsr3u1MHTva49ifr71Eg4IbeI%3D)

프로그래머와 하드웨어 설계자들은

**높은 메모리 지연 시간으로 인한 문제를 해결하기 위한 다양한 기술**을 개발해왔고, 이 기술들은 보통 가지 전략 중 하나 이상에 집중한다.

**1. 더 작은 용량의 고속 메모리를 CPU 코어에 가깝게 배치**하여, 자주 쓰이는 데이터에 **더 빠르게 접근**할 수 있도록 한다.

**2.**메모리 접근이 완료되길 기다리는 동안, CPU가 다른 **유용한 작업을 수행하도록 배치**해 **메모리 지연 시간을 숨긴다 (latency hiding).**

**3.** 프로그램의 데이터를 **작업 흐름에 맞게 효율적으로 배열**하여, **주 메모리 접근 횟수를 최소화**한다.

### 

### **Register Files**

CPU의 **레지스터 파일**은 **메모리 접근 지연 시간을 최소화하기 위해 설계된 가장 극단적인 형태의 메모리 구조**다.

레지스터는 보통 **멀티포트 SRAM**으로 구현되며, **읽기/쓰기 포트를 따로** 둬서 **동시에 처리**할 수 있게 되어 있다.

또한, **레지스터 파일은 ALU(산술 논리 장치)** 바로 옆에 배치된다.

memory hierarchy를 보면 cpu register가 최상단에 배치되어있는 것을 볼 수 있다.

![](https://blog.kakaocdn.net/dna/b3xscP/btsPFYD7EuH/AAAAAAAAAAAAAAAAAAAAAG_3qvbA6fGUJGtv0tItkVVFCM2m3_thH7pJKo1l8sWw/img.webp?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1772290799&allow_ip=&allow_referer=&signature=VcffwUbQgRjzzoxiL8QwlA5MzEw%3D)

ALU는 **거의 직접적으로 레지스터에 접근**할 수 있는 반면,   
**주 메모리에 접근하려면** 다음과 같은 경로를 거쳐야 한다.

1. 가상 주소-물리 주소 변환 시스템 (MMU가 TLB 체크하고, miss가 발생하면 page를 조회한다.)

2. 메모리 캐시 계층 (L1, L2, L3 cahche에 존재하는 지 cache hit을 검사한다)

3. 캐시 일관성 프로토콜 (멀티 코어 환경에서는 서로의 cache의 일관성을 체크해줘야한다.)

4. 주소/데이터 버스 (cache miss가 난다면 이제 main memory로 찾으러 가야한다. 여기도 없으면 디스크로 가던가...)

5. 크로스바 스위치 같은 복잡한 경로

이러한 이유 때문에 **레지스터 접근은 매우 빠르며**, **레지스터 메모리는 일반 메모리보다 훨씬 비싸다**.

### 

### Memory Cache Hierachies

메모리 캐시 계층 구조는 오늘날의 개인용 컴퓨터와 게임 콘솔에서 높은 메모리 접근 지연(latency)의 영향을 완화하기 위한 주요한 메커니즘 중 하나다.

이 계층 구조에서는 **CPU 코어와 매우 가까운 위치(같은 다이 내)에 L1 캐시(Level 1 cache)라는 작지만 빠른 RAM 뱅크가 배치된다.****L1 캐시는 CPU의 레지스터 파일만큼이나 접근 지연이 낮을 정도로 가깝다.**

일부 시스템은 이보다 더 크지만 약간 느린 L2 캐시(Level 2 cache)를 제공하는데, 이는 코어보다 더 먼 곳(대개는 여전히 칩 안에 있고, 종종 여러 코어가 공유)에서 동작한다.

또 어떤 시스템은 **L3 또는 L4 캐시**처럼 더 크지만 더 느린 캐시도 가지고 있다.

**이러한 캐시들은 통해, 자주 사용되는 데이터를 자동으로 보존함으로써, 메인보드에 위치한 매우 크지만 매우 느린 메인 메모리(main RAM)에 접근해야 하는 횟수를 최소화한다.**

**캐싱 시스템은 자주 접근되는 데이터의 지역 복사본(local copy)을 캐시에 보관하여 메모리 접근 성능을 향상시킨다.**

CPU가 요청하는 데이터가 이미 캐시에 있을 경우, 수십 사이클(cycle) 내로 매우 빠르게 제공된다. **이것을 캐시 히트(cache hit)라고 한다.**

**반면, 요청한 데이터가 캐시에 없으면 메인 메모리에서 캐시로 불러와야 하며, 이 경우를 캐시 미스(cache miss)라고 한다.**

**=>메인 RAM으로부터 데이터를 읽는 데는 수백 사이클이 걸리므로, 캐시 미스는 매우 큰 성능 비용을 초래한다.**

### 

### **Cache Lines**

메모리 캐시는 실제 소프트웨어에서 자주 발생하는 두 가지 **참조 지역성(locality of reference)** 특성을 활용한다

**1. 공간적 지역성 (Spatial locality)**  
어떤 메모리 주소 N이 접근되었다면, 그와 가까운 주소들(N+1, N+2 등)도 곧 접근될 가능성이 크다.

예를 들어 배열을 순차적으로 탐색하는 패턴은 공간적 지역성이 높은 접근 방식이다.

**2. 시간적 지역성 (Temporal locality)**  
어떤 메모리 주소 N이 접근되었다면, 가까운 미래에 동일한 주소가 다시 접근될 가능성이 크다. 변수에서

데이터를 읽고, 변환한 뒤 다시 같은 변수에 결과를 저장하는 방식은 시간적 지역성이 높은 예다.

(=> 최근에 사용한 데이터는 캐시에 남아 있을 확률이 높으니 좋은 방법이다.) 

**이러한 지역성을 활용하기 위해,**

**메모리 캐싱 시스템은 데이터를 개별 항목 단위로 저장하는 것이 아니라 연속된 블록 단위(cache line)로 캐시에 옮긴다.**

예를 들어 클래스나 구조체의 멤버 데이터를 읽는 경우를 생각해보자.

첫 번째 멤버를 읽을 때는 메모리 컨트롤러가 메인 RAM에서 데이터를 가져오느라 수백 사이클이 걸릴 수 있다.

하지만 이때 **캐시 컨트롤러는 해당 멤버 하나만 읽어 오는 것이 아니라, 더 큰 연속된 메모리 블록을 한꺼번에 캐시에 가져온다.**

이렇게 하면 다음 멤버를 읽을 때는 **cache hit**이 발생하여 빠르게 처리된다.

### 

### **캐시 라인을 메인 RAM 주소에 매핑하기**

캐시의 메모리 주소는 메인 RAM의 주소와 **단순한 일대다(one-to-many)** 방식으로 대응된다.

캐시 주소 공간은 메인 RAM 주소 공간에 대해 반복적으로 매핑되어 있다고 볼 수 있다.

다시 말해, 메인 RAM의 주소가 0부터 시작해서 끝까지 커버되도록 캐시가 반복해서 대응된다.

![](https://blog.kakaocdn.net/dna/5iIUW/btsPF692ZOM/AAAAAAAAAAAAAAAAAAAAADGMCZRuADnBvDIgq6kmH7dCw_m4NQisWxwYS2uruhq5/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1772290799&allow_ip=&allow_referer=&signature=ZMyatwKb5fMHZAbNMI2mn9DMivk%3D)

예를 들어, 캐시가 32KiB 크기이고, 캐시 라인이 128바이트라고 가정하자. 그러면 이 캐시는 총 **256개의 캐시 라인**을 담을 수 있다 (256 × 128 = 32,768B = 32KiB). 메인 RAM이 256MiB라면, 이는 캐시보다 **8192배 더 크다**.  
(256 × 1024) / 32 = 8192

즉, 메인 RAM 전체를 커버하기 위해 캐시 주소 공간을 8192번 반복해서 덮어야 한다. 다시 말하면, **캐시의 한 라인은 메인 RAM의 8192개 서로 다른 블록에 대응될 수 있다**.

메인 RAM의 특정 주소가 캐시의 어떤 주소에 대응되는지는 다음과 같이 계산할 수 있다:

**캐시 주소 = (메인 RAM 주소) % (캐시 크기)**

### 

### **Addressing the Cache**

CPU가 메모리에서 단일 바이트를 읽으려고 할 때 어떤 일이 벌어지는지 살펴보자.

1. 메인 RAM에서 원하는 **바이트의 주소가 캐시 내의 주소로 변환된다**.  
2. 캐시 컨트롤러는 해당 바이트가 포함된 **캐시 라인**이 이미 **캐시에 존재하는지 확인한다**.

3-1. **존재한다면**(Cache Hit) 해당 바이트는 캐시에서 읽어온다.

3-2. **존재하지 않는다면**(Cache Miss) **메인 RAM에서 라인 크기(line-sized chunk)** 만큼의 데이터를 읽어와 캐시에 적재한다.

(line size로 가져오기에 캐쉬 주소는 line size로 정렬되어야한다.

이렇게 하면 이후에 근처 주소를 읽을 때 더 빠르게 처리할 수 있다.

사실 Cache Miss 가 났을 때 추가되는 작업이있다.

Cache Miss가 발생하면, 캐시 컨트롤러는 메인 RAM에서 라인 크기만큼의 데이터를 읽어 **해당 라인 인덱스**에 저장한다는 것 까지 지금까지 잘 배웠다.

여기서 추가 작업이 존재하는데.  
**그 라인이 메인 메모리의 어떤 블록에서 왔는지**를 **추적하기 위해 태그 테이블(tag table)에 태그를 기록한다.**

이는 **캐시와 메인 메모리 간의 다대일(Many-to-One) 관계**를 관리하기 위한 필수적인 과정이다.

(1개 캐시 메모리에 n개의 메인 메모리의 데이터가 올 수 있다. 이를 어느 곳에서 왔는 지 구별하기 위한 Tag라고 이해하면 된다.)

### 

### 최종 정리를 해보자

1. CPU가 읽기 요청을 수행한다.

2. 메인 메모리 주소는 다음 3가지로 분할된다:

**오프셋(offset)**: 캐시 라인 내부에서 바이트의 위치

**라인 인덱스**: 캐시 내 위치

**태그(tag)**: 어떤 메모리 블록에서 왔는지 식별

3. 캐시 내 태그 테이블을 라인 인덱스로 탐색해 일치하는 태그가 있는지 확인한다.

**태그가 일치**하면 => **Cache Hit =>**라인 인덱스를 통해 해당 라인을 읽고, 오프셋으로 바이트 위치를 찾는다.

**태그 불일치**하면 => **Cache Miss =>** 메인 RAM에서 라인 전체를 읽어 캐시에 저장하고, 태그 테이블도 갱신한다.

(이후 해당 캐시 라인 내 인접한 바이트들을 읽는 경우에는 **캐시 히트**가 발생해 매우 빠른 접근이 가능하다.)

### 

### **Set Associativity**

지금까지 설명한 방법은 Direct-Mapped Cache이다.

이는 메인 RAM의 각 주소가 캐시의 단 하나의 라인에만 매핑된다는 의미이다.

이 방법의 문제는 Cache miss가 났을 때이다 .

캐시 미스가 발생하면, CPU는 해당 캐시 라인을 메인 메모리에서 가져와야 한다.

해당 캐시 라인이 **비어있다면** 단순히 데이터를 복사하고 끝나지만,

하지만 **이미 다른 블록의 데이터가 들어있다면,** 그 데이터를 **덮어써야 한다**.

그게 무슨 문제냐고 생각할 수 있찌만,

ping-pong같이 서로를 번가라가면서 계속 덮어씌울 가능성이 존재한다는 것이다.

이를 해결하기 위해서

Set Associative Cache 라는 새로운 방법이 도입되었다.

**하나의 메인 메모리 주소가 여러 개의 캐시 라인 중 하나에 매핑**되도록 만들 수 있다.

예를 들어 **2-Way Set Associative Cache**에서는

하나의 메인 메모리 주소가 **두 개의 캐시 라인(way)** 중 어느 하나에 매핑될 수 있다.  
 즉, **충돌이 나도** **둘 중 비어있는 곳을 선택할 수 있다.**

![](https://blog.kakaocdn.net/dna/cQHTvW/btsPGFYQCgw/AAAAAAAAAAAAAAAAAAAAAND8X7HN42gHYxGoVxws6RmHGpnZpcSJpgb3aRLCwfsW/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1772290799&allow_ip=&allow_referer=&signature=NPBpsUJvCom9u7Ag%2B4%2FhA%2FDdpCA%3D)

(4-way, 8-way도 존재하고, 연관도를 높이면 평균 성능이 좋게 나온다.)

2-WAY일 때 어떤 Cache Line을 덮어쓸지는 CPU의 설계에 따른다.

### 

### **명령어 캐시(Instruction Cache)와 데이터 캐시(Data Cache)**

게임 엔진이나 성능에 민감한 시스템을 위한 **고성능 코드**를 작성할 때,

반드시 알아야 할 점은 **데이터와 코드 모두 캐시에 저장된다는 것**이다.

**명령어 캐시(I-cache 또는 I$)**  
실행될 기계어(machine code)를 **미리 불러오기(preload)** 위해 사용된다.

**데이터 캐시(D-cache 또는 D$)**  
기계어가 수행하는 **읽기/쓰기 연산을 빠르게 처리**하기 위해 사용된다.

L1 캐쉬에서는 I-Cache와 D-Cache는 물리적으로 분리되어있다.

=> 명령어를 읽는 작업이 데이터 캐시를 덮거나, 데이터 접근이 명령어 캐쉬를 덮는 일을 피하기 위함

하지만, 상위 캐시들(L2, L3, L4)에서는 보통 코드와 데이터의 구분이 **명확히 나뉘지 않는다**.  
이유는 L1에 비해서 **크기가 크기 때문에 덮어씌우는(eviction)이 잘 발생하지 않기 때문**이다.

### 

### **Write Policy (쓰기 정책)**

지금까지는 CPU가 읽기(Read)를 수행할 때만 다뤘다.  
그렇다면 CPU가 쓰기(Write)를 할 경우는 어떻게 처리될까?

#### 1. Write-Through Cache

가장 단순한 방식

캐시에 쓰기와 동시에 **메인 RAM에도 즉시 동일한 데이터를 반영**함

**신뢰성은 높지만 느림**

#### 2. Write-Back (또는 Copy-Back) Cache

데이터는 먼저 **캐시에만 쓰이고**, 나중에 필요할 때만 메모리에 반영됨

**Dirty Line**이 교체되어야 할 때

혹은 프로그램이 **명시적으로 flush**를 요청할 때

**성능은 높지만, 복잡성과 동기화 문제가 생길 수 있음**

### 

### **캐시 일관성(Cache Coherency)**

CPU 코어가 여러 개 존재하고 **하나의 메인 메모리 영역을 공유**하는 시스템에서는 상황이 더 복잡해진다.

보통 **각 코어는 자신의 L1 캐시**를 가지고 있고,

**L2 캐시나 메인 RAM은 여러 코어가 함께 공유**한다.

이때 중요한 문제는 바로 **캐시 일관성(Cache Coherency)**이다.

![](https://blog.kakaocdn.net/dna/G1lbI/btsPE9l5Wce/AAAAAAAAAAAAAAAAAAAAAOKeYVa4Db3uM3ycbJhI405zSpOHxA25XrZ1qLLAF_dF/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1772290799&allow_ip=&allow_referer=&signature=cKtnimEvnVQ86bRCikY5PP7oFag%3D)

**여러 코어의 캐시와 메인 메모리 간의 데이터가 서로 불일치하지 않도록 유지하는 것**이다.

완벽하게 항상 일관성을 유지할 필요는 없지만,

프로그램에서 캐시들의 불일치를 체감, 느끼면 안된다.

### 

### **Avoiding Cache Misses**

결국 어떤 데이터든 **언젠가는 메인 RAM으로부터 이동해 와야 하기 때문에 Cache Miss를 완전히 피하는 것은 불가능하다**.

하지만 고성능 소프트웨어를 작성할 때의 핵심은, **캐시 계층 구조를 고려하여 RAM에 데이터를 배치하고, 데이터 조작 알고리즘을 설계하는 것**이다.

즉, **캐시 미스를 최소화**하는 방향으로 구조를 짜야 한다.

#### 

#### D-cache Miss를 줄이는 가장 좋은 방법

1. 데이터를 메모리에 연속적으로 저장(contiguous)

**2. 데이터 크기를 가능한 작게 유지**하며,

3. 연속적으로 순차 접근(sequential access)

이러한 조건을 만족하면

하나의 캐시 미스로 **관련 데이터 전체를 한 번에 불러올 수 있고**,

데이터가 작다면 **하나의 캐시 라인에 전부 들어갈 수도 있으며**,

반복 접근 시 **같은 캐시 라인이 계속 유지되어 재로딩을 피할 수 있다.**

#### 

#### I-cache Miss를 피하는 법도 같은 원리지만, 구현 방식은 다르다:

1. 고성능 루프는 **가능한 한 코드 크기를 작게 유지**하라.

2. 루프 내부에서 **함수 호출은 피하거나**,  
꼭 호출해야 한다면 **그 함수들도 작게 유지**하라.

3. 이로 인해 **루프 전체가 캐시에 머무르며**, 실행 성능이 향상된다.

단 inline 함수는 신중하게 사용해야된다.

작은 함수라면 inline이 큰 성능 이점을 줄 수 있지만, 너무 많은 함수를 inline 처리하면 코드 크기가 커저 I-Cache룰 넘게 되어 성능이떨어질 수 있다.

### 

### **Nonuniform Memory Access, NUMA**

멀티프로세서 게임 콘솔이나 개인용 컴퓨터를 설계할 때,  
시스템 아키텍트는 다음 **두 가지 근본적으로 다른 메모리 구조** 중 하나를 선택해야 한다:

**1. UMA (Uniform Memory Access)**

**2. NUMA (Nonuniform Memory Access)**

#### 

#### **UMA**

시스템 전체에 **하나의 큰 RAM 뱅크**가 존재한다.

**모든 CPU 코어는 동일한 메인 메모리에 접근**할 수 있으며, **물리적 주소 공간도 모든 코어에서 동일하게 보인다.**

보통 UMA 아키텍처는 **캐시 계층 구조**를 활용해  
**RAM 접근 지연(latency)** 문제를 줄이려 한다.

#### 

**문제**

**여러 코어가 메모리나 캐시를 두고 경쟁**하게 되는 문제가 있다.

예: PS4

결과적으로 **L2 캐시와 메인 RAM을 두고 경쟁이 자주 발생**한다.

#### 

#### **NUMA (비균일 메모리 접근)**

이러한 경쟁 문제를 완화하기 위해 도입되는 것이 **NUMA 구조**다.

각 코어는 전용 고속 RAM (local store)을 가진다.

이 local store는 마치 L1 캐시처럼 **해당 코어 전용**이고, 같은 다이(die)에 위치한다.

하지만 **L1 캐시와의 차이점**은 다음과 같다:

1. **Local store 접근은 자동이 아니라 "명시적(explicit)"이다.**

2. local store는 특정 주소 영역에 직접 **맵핑되거나**,  
3.혹은 DMA 컨트롤러(DMAC)를 통해 메인 RAM과 데이터 교환이 일어나도록 설계된다.

![](https://blog.kakaocdn.net/dna/FVjmN/btsPErOs8AO/AAAAAAAAAAAAAAAAAAAAAAqKv-grJe_dIzCWWyNGK26jS_CW-TLN3r8RcaixFe2B/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1772290799&allow_ip=&allow_referer=&signature=6XfywrLX9k9Em90dpkJPWpkdO9E%3D)