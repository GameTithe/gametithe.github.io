---
title: "[딥러닝] MLP - 2"
date: 2025-03-14
toc: true
categories:
  - "Tistory"
tags:
  - "tistory"
---

MLP-1입니다

## **<https://tithingbygame.tistory.com/166>**

[[딥러닝] MLP(Binary, Multiclass, Multilabel Classification) - 1

Classification Binary Classification: 강아지가 있는 지 없는지 (0 or 1)Multicalss Classification: 여러개의 옵션 중 1개를 선택Multilabel Classification: 여러개가 동시에 있을 수 있음 Perceptronf = activation function (step f

tithingbygame.tistory.com](https://tithingbygame.tistory.com/166)

## **MLP(Multi Layer Perceptron)**

perceptron / logistic regresion은 XOR gate problem을 sigmoid activation이 적용된 hidden layer를 추가해서 해결했다.

좌측: xor gate problem

우측: hidden layer를 통해 우측처럼 변화시키면 분류가능

![](https://blog.kakaocdn.net/dna/dbnLJe/btsMKoM75H8/AAAAAAAAAAAAAAAAAAAAAMTEI9_kKTP4ewRcslo-N6PbKXe-u5fS7hk6OwJNSJjA/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1772290799&allow_ip=&allow_referer=&signature=MgfyifpNfH2aGt4GPzkvHkruvB4%3D)![](https://blog.kakaocdn.net/dna/PRXsu/btsMLuy6tma/AAAAAAAAAAAAAAAAAAAAABBqF53YAjJm4NjEc6jbB1Q5uFzgMqVPi3_wqFb--q9u/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1772290799&allow_ip=&allow_referer=&signature=lsqXGjtWUC6%2By%2FkESmAZhO%2BLBNI%3D)

MLP는 Full Connection으로 만들어진다.

내가 원하는 node들만 이어서, 원하는 예측을할 수 있는것이 아니라 full connection이기 때문에 사람이 예측하기 어렵다.

![](https://blog.kakaocdn.net/dna/ma08e/btsMKwRUq2d/AAAAAAAAAAAAAAAAAAAAAFjrbKaNsZvOpszBT2xqVEGjltirguJZOuj-lPGQbkdi/img.jpg?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1772290799&allow_ip=&allow_referer=&signature=IWAPhR7e4SdYyo883EA8ukGcoN0%3D)

### **Forward Propagation**

![](https://blog.kakaocdn.net/dna/chZ4x0/btsMKecYVWu/AAAAAAAAAAAAAAAAAAAAAH_7hCgQw8CbUQPYuwDw27GbnC8j941fixsCWtfY8Qp3/img.jpg?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1772290799&allow_ip=&allow_referer=&signature=RD8yZ%2BX9KTFeEVlg%2BJhqdKevPIE%3D)

Ci = i번째 hidden layer Li의 node, neuron, unit, feature (4개의 단어 모두 동일한 의미)의 수 이다.

### **Paramerter 수**

parameter의수는 = 선의 수라고 볼 수 있고, 구하는 방법은 다음과 같다.

첫 번째 layer의 파라매터 수를 구하는 법이다. 이것을 다음 layers에도 적용시켜주면 구할 수 있다.

![](https://blog.kakaocdn.net/dna/ddpk2B/btsMLyauhM3/AAAAAAAAAAAAAAAAAAAAANGK7UqzqYxUlx22l3E5oPpAXzizDJtFfcve3pNk64Tz/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1772290799&allow_ip=&allow_referer=&signature=C8TMAfQyZ8i2QLSWFkKFhzyEetA%3D)

Ci-1  \* Ci + Ci 인데 이를 정리하면, (Ci-1 + 1)\* Ci 이다.

(6 + 1)\*7 + (7 + 1) \* 4 + (4 +1 ) \* 1 or

(6 \* 7) + 7  + (7 \* 4) + 4 + (4 \*1) + 1 이다.

### 

### **Backpropagation**

우리가 지금까지 weight, bias를 통해서 output을 추출했는데, 이게 1번 순회했다고 정답이 나오는게 아니다.

weight를 계속 정답에 맞게 바꿔주면서 학습을 하는 시간이 필요한 것이다.

**backpropagation은 weight를 알맞게 조정하기 위한 작업니다.**

말로 설명하는데는 한계가 있고, 이 영상을 보면 많은 도움이 될 것 같습니다... ㅠㅠ

<https://www.youtube.com/watch?v=1Q_etC_GHHk>



### 

### **Activation Function**

Activation Function을 사용해야 되는 이유는 이전 글에서 설명했지만, 다시 설명하면

만약 Activation Function을 사용하지 않고, 선형 형태를 유지한다고 가정하자.

그리고 3개의 레이어를 지났다고 하면 다음과 같이 표현할 수 있을 것이다. =>w 1\* w2\* w3 \* x1

근데 이게 선형이라면 w 1\* w2\* w3 \* x1   == w3 \* w2 \* w1 \* x1 이다.

layer의 순서가 중요하지 않게 되고, 사실 1개의 layer를 통과한 것으로 가정된다. 그렇기 때문에 비선형으로 변환이 되어야한다.

그래서 우리가 Sigmoid / Tanh  함수를 사용했다. 하지만 여기에도 vanishing gradient(기울기 손실) 라는 문제가 존재한다.

**backpropagation은 activation function의 gradient들이 계속 곱해지는데,**

**그래프를 보면, 각 activation function의 gradient를보면 함수의 값보다 작아지고 있다.**

**layer를 거칠 수록 값이 작아져, input에 가까워 질 수 록 0에 가까워지고 옳바른게 weight를 수정해주지 못한다.**

좌측이 sigmoid 함수이고, 점선이 gradient이다.

우측이 tanh함수이고, 점선이 gradient이다. 보시다시피 대부분의 gradient의 값은 1보다 작다.

![](https://blog.kakaocdn.net/dna/NQXj9/btsMLvSzxvD/AAAAAAAAAAAAAAAAAAAAABKm4Wd56_9iBI87IzUpDg3lqcuR_AUot5hNxW2Bz8qY/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1772290799&allow_ip=&allow_referer=&signature=Sa50lPxdCI1ArMBbyUK9CcUQH0U%3D)![](https://blog.kakaocdn.net/dna/S5aXJ/btsMJxqTgZz/AAAAAAAAAAAAAAAAAAAAAAvpvXwpIHUcumjvKhU3iT34iFKcxBgpHC65_BTxKOi8/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1772290799&allow_ip=&allow_referer=&signature=BBypyEvkVVqtRGfj1dBo4CS52v8%3D)

이를 해결하기 위해서 ReLU 함수를 사용해야된다.

파란색 선이 gradient인데, ReLU의 gradient는 0 or 1로 gradient를 보존 가능하다.

+ Sparsity  ( 0보다 작으면 그냥 0으로 바꿔준다. 필요한 정보, 필요없는 정보를 취사 선택하기 유용하다. 필요없으면 음수로 보내면 된다. )

+ 0 or 1 이니 계산도 편함

![](https://blog.kakaocdn.net/dna/MzE1D/btsMKu1cQfs/AAAAAAAAAAAAAAAAAAAAAGHK3ylirp985W2CrvIkpa9c47xFzjmwk8VTaPN3xgnb/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1772290799&allow_ip=&allow_referer=&signature=SVLtgT0HAPmXK7%2F%2FnZDTGTcLSeM%3D)

그럼 장점만 있냐?

1. Activation 값들이 양수만 가지기 때문에 평균이 0이 아니다. (위의

2. Exploding Gradient 문제: 값이 한 번 커지면 계속 커질 가능성이 있음

3. Dead Node: 한 번 0이 된 노드와 연결된 노드들 또한 0이 되기 때문에 해당 sample에 대해 학습할 수가 없어짐 (Leaky ReLU, GeLU, Swish로 보완 가능)

### 

## **Optimization**

**전체 Optimization 과정( w\* 를 찾는 과정 => 최적의 w값)을 Model Training이라고 한다.**

선형 회귀, 로지스틱 회귀 같은 단순한 convex 문제는 global mininum을 찾을 수 있었지만,

layer가 깊어지면서 global minimum을 보장할 수 없게 되고, local minimum을 찾는 방향으로 바뀌었다.

![](https://blog.kakaocdn.net/dna/p8jUR/btsMJ5nn0hi/AAAAAAAAAAAAAAAAAAAAAHw4ba0gLgQMLPJ_35Gj17HdsdtSLBGYtv7gcLolTIuZ/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1772290799&allow_ip=&allow_referer=&signature=YZm8Dt5w7cg%2BTlJzeMc7Flp3ufc%3D)

### **Gradient Descent**

Gradient Descent는 **Loss function의 gradient에 learning rate를 곱해서 구한다.**

![](https://blog.kakaocdn.net/dna/ccWKkB/btsMLPpSNwk/AAAAAAAAAAAAAAAAAAAAACdOHx9ZlRLooFz0IbZqf0xWrYWe80Bhw122vDZmhDjE/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1772290799&allow_ip=&allow_referer=&signature=RDKYwA3nPLyM4nzz5B8l54OSt2A%3D)

**m = 전체 학습 데이터 일 때, (Batch Gradient Descent)**

전체를 학습데이터로 사용하면 **정확한 gradient를 얻을 수 있지만 연산량이 커진다.**

**m = 1 일때**

한개의 sample에 대한 cost function을 계산해 **연산량이 적지만, gradient가 부정확해진다.**

**m = mini batch gradient descent ( 2 ^ n )**

**mini batch 크기에 따라 연산량 & 학습 안정성이 결정된다.**

학습량과 안정성이 비례한다고 볼 수 있다.

아래의 그림을 보면 쉽게 이해할 것이다.

Batch Gradient는 local minimum을 향해 곧바로 간다.

min-batch는 삐뚤빼뚤하지만 곧잘 간다.

하지만 stochastic gradient descent (m == 1) 는 길을 굉장히 길을 해매고 있다.

![](https://blog.kakaocdn.net/dna/cxL7fx/btsMKedqqSH/AAAAAAAAAAAAAAAAAAAAAHzNMnPBYm2Kk2_J1Np1uZZoOF-y9e03BZwaeiR8uNoC/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1772290799&allow_ip=&allow_referer=&signature=mgrjW1rA1XoByY6eIRtNzHiHlgs%3D)

#### 

그럼 어떤 Gradient Descent를 사용할까?

**1. Batch Gradient Descent**

시간이 오래걸림..

그리고  위의 그림처럼 local minimum을 잘 찾아감 근데,, 그게 global minimum이라는 보장이 없음. 너무 잘 찾아가도 문제.. + sharpf minimum

sharp minimum은 아래 그림과 같이 gradient가 큰 곳으로 이동했지만, 오히려 train과 test의 차이가 큰 곳이다.

![](https://blog.kakaocdn.net/dna/cTCNCL/btsMKSHGKxJ/AAAAAAAAAAAAAAAAAAAAAAShaltw923pvJ0Z_S2Wn9JLG-52oq52OzaCEUP-L_o7/img.jpg?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1772290799&allow_ip=&allow_referer=&signature=JbrtPaa8%2FTgbuaYwg9BxL05VCZc%3D)

**이러한 이유들로 탈락**

**2. Stochastic Gradient Descent (  m == 1 )**

가끔 사용되는 방법이다. batch 크기를 1보다 크게 했을 때 memory 부족 오류가 뜨면... 어쩔 수 있나 가장 작게해야지..

그리고 m == 1일 때 학습이 잘 되는 경우가 간혹있다고 한다.

위와 같은 상황일 때만 사용되는 방법

**3. Mini-Batch Gradient Descent**

학습 시간 절약 + 학습도 잘 됨

안 쓸 이유가 없다.

3번 Mimi-Batch Gradient Descent 방법을 주로 사용한다.

#### **Epoch**

Epoch : **전체 학습데이터**를 순회하는 단위

**n Epoch: 전체 학습데이터를 n번 순회**

![](https://blog.kakaocdn.net/dna/S0no2/btsMLYtrf5n/AAAAAAAAAAAAAAAAAAAAAP9Nvvx7AFIlLla7opPCSlyag2byM2FHGT1G-LgHPoEl/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1772290799&allow_ip=&allow_referer=&signature=zEtIoRQiBrMDL7LLF8ry5ZeWuU8%3D)

그림 처럼 mini batch를 200을 잡으면 mini batch를 10번 순회해야지 1 Epoch라고 할 수 있다.

## 

## **Optimizer**

### **1. SGD ( Stochastic Grdient Descent, gradient descent에서 말한 SGD에서 확장된 의미 )**

**gradient descent를 사용한다는 것은 동일하다.**

![](https://blog.kakaocdn.net/dna/cfnKFj/btsMKqSmZyF/AAAAAAAAAAAAAAAAAAAAAOeiE5XdgFeEzpXNgDxvEeOWysgmXcx-Z2Pauhmu7OOT/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1772290799&allow_ip=&allow_referer=&signature=KVaWP3XSOMLr4tLzKSblfe7ynYQ%3D)

**근데 Momentum을 같이 사용한다.**

**Momentum( β ): 이전 gradient와 현재 gradient의 가중평균(이동평균)을 사용한다.( 섞어서 사용한다. )**

β = 0.9를 많이 사용한다.

이전 gradient를 섞어서 사용하면 아래의 그림처럼 방향을 잘 잡는다.

![](https://blog.kakaocdn.net/dna/bEL2xD/btsMKOyKyy5/AAAAAAAAAAAAAAAAAAAAAAbEAEa-kvMCv2zJwo8qybz6Rp18gHnJNsNhWC_v7BGV/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1772290799&allow_ip=&allow_referer=&signature=72TqpURh5Nje3KvMAtLGPws7qa0%3D)

 이동평균(1st moment)을 구하는 방법이다. 단순한 보간 방법이다. (loss function 미분 = gradient) 

![](https://blog.kakaocdn.net/dna/6XLhU/btsMJyp4mzQ/AAAAAAAAAAAAAAAAAAAAAMBub0ECZj3inPMQZOff0TrwRzaLgYoPXbAONacBWqy9/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1772290799&allow_ip=&allow_referer=&signature=Z0i%2BCpS0D%2BhlrAycElE8TftTl6A%3D)

### **2. Adam**

1st moment( β1)+ 2nd( β2) moment도 사용한다.

1st moment는 동일하다.

![](https://blog.kakaocdn.net/dna/6XLhU/btsMJyp4mzQ/AAAAAAAAAAAAAAAAAAAAAMBub0ECZj3inPMQZOff0TrwRzaLgYoPXbAONacBWqy9/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1772290799&allow_ip=&allow_referer=&signature=Z0i%2BCpS0D%2BhlrAycElE8TftTl6A%3D)

2nd momnent는 **gradient를 제곱한 이동 평균**이다.

쉽게 말해 **현재까지** **gradient가 얼마나 변화했는지에 대한 정보이다.**

(parameter 현재까지 얼마나 많은 영향을 끼쳤는지)

![](https://blog.kakaocdn.net/dna/bZvcnA/btsMLJi9qh8/AAAAAAAAAAAAAAAAAAAAADK2_h6J0PVuLVClSBUKk9nSEp_MFR8CbKt_refwmm21/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1772290799&allow_ip=&allow_referer=&signature=KLFKLsyl9mZhAGxPvP9bC0VV5GU%3D)

우리는 v0, s0 초기값을 평균내고 싶어도 값이 없으니 0으로 초기화를 하게 될 것이다. 그렇게하면 데이터가 bias하게 설정된다.

이를 방지하고자 아래와 같은 방법으로 초기화를 해준다.

![](https://blog.kakaocdn.net/dna/rE4r4/btsMLXuHH9H/AAAAAAAAAAAAAAAAAAAAAMgs3SBLZSyTomAzRoU6g0pjCZE4QPNP6a-BJzWAzsWr/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1772290799&allow_ip=&allow_referer=&signature=h62gsgWK%2FmY09pVEPLvhFEcHGRc%3D)

2nd moment를 역수로 주면 현재까지 학습이 많이된(값이 큰) parameter는 영향을 적게, 학습이 적게 된(값이 작은) parameter는 영향을 많이 줄 수 있을 것이다. 아래 식이 최종 Adam optimizer식이다.

![](https://blog.kakaocdn.net/dna/nWaAR/btsMLy3aUjf/AAAAAAAAAAAAAAAAAAAAAMYmYN6N4Rtlnt3A1LbWy_KmY2HJYGUoEJYMKFQy1aJQ/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1772290799&allow_ip=&allow_referer=&signature=ZQSyjrvE%2Bbs3l33EFhIJcfmoo9A%3D)

(베타들은 대부분 수정하지 않고, .. β1=0.9, β2 = 0.999로 사용한다. 입실론은 10^-8을 많이 사용하지만 변경도 많이드라...)

### 

## **Regularization**

모델의 bias를 증가시켜서, 분산을 크게 만들고 이를 통해 overfitting을 방지한다

### **1. L1 Regularization**

불필요한 weight를 0으로 수렴시킨다.

![](https://blog.kakaocdn.net/dna/ukYTL/btsMLrDbMYV/AAAAAAAAAAAAAAAAAAAAAJ5y01F_rEgD0tKZoi5yH7qAZX5zt6_3ODtLGRWZW4jf/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1772290799&allow_ip=&allow_referer=&signature=l2qstbk1TCSgXSKA4MAbs7tW9Fg%3D)![](https://blog.kakaocdn.net/dna/c60sCE/btsMK3P2DdR/AAAAAAAAAAAAAAAAAAAAAPdgs0N-O2texSA5NEsTev5aOYoMvGolua5Kr03YAJJx/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1772290799&allow_ip=&allow_referer=&signature=bkHxL9ML0hdDR7ELym40%2FOo8ij4%3D)

SGD 함수에 적용시켜보면

![](https://blog.kakaocdn.net/dna/bbJ4Ng/btsMLw5qPvE/AAAAAAAAAAAAAAAAAAAAADgXcg4YgCGTAI4R5vi-tjtUAXFNI10EsY5Dx8sNobyI/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1772290799&allow_ip=&allow_referer=&signature=jTT8O1KBv61U8FUPhvFLurrzjcs%3D)

sgn(wi​): 부호 함수로 wi > 0 이면 1, wi < 0이면 -1 이기 때문에

**불필요한 파라미터 wi​들을 0으로 수렴**하게 만드는 효과를 얻을 수 있다. ( Sparsity )

### **2. L2 Regularization**

지나치게 큰 weight를 억제할 수 있다.

![](https://blog.kakaocdn.net/dna/BkHRC/btsMKsW4Hem/AAAAAAAAAAAAAAAAAAAAAKaxIOQKiURnKCGltioK0uMGXe1fSkEb21byxT3MJXdC/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1772290799&allow_ip=&allow_referer=&signature=k4JlSsuFQoInPaHa2G3cCZar9oc%3D)![](https://blog.kakaocdn.net/dna/cqvJSP/btsMLuzQame/AAAAAAAAAAAAAAAAAAAAAJNV4mVCuNdfIUY-7pQ_N2FUvdxQTOutuutlb83VMXUG/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1772290799&allow_ip=&allow_referer=&signature=xY431tTo%2FPqqLmPFnZKcBrfgNAw%3D)

SGD 함수에 적용시키면 아래와 같은데 딱 봐도 w만큼 뺴주는게 억제해주고 있다...!

![](https://blog.kakaocdn.net/dna/bJhMmH/btsMKSOCMV2/AAAAAAAAAAAAAAAAAAAAALRzywqF_7gzJb7R_QXlmrsXJwMG0s-wnpkTkdQN54fQ/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1772290799&allow_ip=&allow_referer=&signature=BDWxox1vyx0HVlUHv5B4N3S7J0A%3D)

Adam에서도 비슷한 방법으로 AdamW가 사용된다. ( 굉장히 자주 쓰이는 optimizer이다. )

### 

### **Learning Rate Scheduling**

**학습이 진행이 될 수록 learning rate를 줄여나가는 방법.**

decay(왼쪽): 학습이 진행될 수록 learning rate를 줄인다.

Warmup(중간): 초기 학습 안정성을 위해서 learning rate를 서서히 증가시킨다.

Restart(오른쪽): local mininum에서 벗어나기 위해, decay 초기화

![](https://blog.kakaocdn.net/dna/qypMp/btsMLQvVtDD/AAAAAAAAAAAAAAAAAAAAAHepbFpBi5gr3KW0jifvmmuf_fO4clQR1wak4hDTQ7A5/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1772290799&allow_ip=&allow_referer=&signature=bEBEG0Gi4UXq5H9%2B9fMJ79lVNhY%3D)![](https://blog.kakaocdn.net/dna/brLpCb/btsMKRvFO2P/AAAAAAAAAAAAAAAAAAAAAHDdD5NNg-fTEGfk4djhy43WJgKBxYve4o1kjG7zoJWd/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1772290799&allow_ip=&allow_referer=&signature=1tFzNzDfL4HLHYLsYp6rwzUsEXk%3D)![](https://blog.kakaocdn.net/dna/bIFcLP/btsMKxqySKP/AAAAAAAAAAAAAAAAAAAAAAlkJYRdYNXs4E9CNx74GlRJFoazZ2e5s4w3JkbZjSCk/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1772290799&allow_ip=&allow_referer=&signature=69bErGgYYSozftD6B7XcVKlFH00%3D)

### **Hyperparameter**

반복적인 실험을 통해 최적의 hyperparameter를 찾는 것이 중요하다.

batchs\_size, epochs, lr., hidden layer의 수, layer당 neural 수 등등