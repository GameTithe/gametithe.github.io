---
title: "[딥러닝] RNN_기본(1)"
date: 2025-03-19
toc: true
categories:
  - "Tistory"
tags:
  - "tistory"
---

### **Tokenization**

text를 컴퓨터에 어떻게 저장할 것인지 나누는 방법

**ASCII**

char = 1 byte로 나타내는 방법

**UTF-8 (Unicode)**

1~4 byte를 사용해서 다양한 방법으로 encoding (ASCII보다 많이 표현가능)

### **Tokenization methods**

text를 token 단위로 나누는 것

모든 token의 집합 ( vocabulary ) =  V

unique한 token의 개수 = |V|

**Chracter-based Tokenization**

```
Input:  "i hate programming"

Tokenized: ['i', ' ', 'h', 'a', 't', 'e', ' ', 'p', 'r', 'o', 'g', 'r', 'a', 'm', 'm', 'i', 'n', 'g']
```

**Word based Tokenization**

```
Input:  "i hate programming"

Tokenized: ['i', 'hate', 'programming']
```

**Subword based Tokenization**

```
input:  "i hate programming"

Tokenized: ['i', 'hate', 'program', 'm', 'ing']
```

### **Byte pair Encoding ( BPE )**

대량의 덱스트 뭉치 (tex corpus)를 구한다.

character based -> corpus내에서 가장 빈번한 pair를 찾는다. ( 2개의 character 쌍 )  -> pair를 새로운 token으로 바꾼다.

위의 과정를 정해놓은 목표 사이즈(vocab size)에 도달할 때까지 반복한다.

**L: 문장의 길이**

**V(vocab size): 모델이 학습하는 단어의 크기**

전체 L의 길이는 점점 줄어들지만, V의 크기는 점점 커질 것이다.

RNN에서는 L이 작아지면 학습 속도가 빨라진다.

V의 크기:  Character-based < Subword-based < word based

**Character-based의 단점**

L이 크기 때문에 학습/추론 속도가 느리다.

**Word-based의 단점**

Out-of Vocabulary문제

( run - runs - rungging - ran 모두 다른 의미로 해석해서 모두 저장해야되는데 vocabulary를 무한히 늘릴 수 없다.  + 희소단어 학습의 문제)

### **Subwrod-based 사용**

Character-based & Word-based의 중간지점 (V값의 sweet spot)  
**단어를 더 작은 단위로 나누면 새로운 단어도 일부 학습된 서브워드를 조합하여 이해할 수 있다고 볼 수 있다**

**=>Inductive bias를 활용한 preprocessing으로 볼 수 있다.**

### **Word Embedding Layer**

Word Embedding Layer는 **단어를 숫자로 변환**하고, 이를 신경망(NN, Neural Network)에서 학습할 수 있도록 만드는 과정이다.  즉 **word embedding + nn**이다.

**Word Embeding**

text data는 continuous data가 아니라, discrete sequence data이다.

**tokenization**을 통해 **encoding**된 **token**은 정수 값을 가지지만, **정수가 수치적으로 비슷한 값을 가지고 있다고해도 비슷한 의미를 가지고 있는 것은 아니다.**

token이 아래와 같은 값을 가질 수 있는데, 이는 수치적인 값이 비슷한 의미를 나타내는 것이 아니라는 예시이다.

```
Hi = "100"
Eat = "101"
Hello" = "1123123"
```

**그럼 Similarity를 어떻게 확인하냐?**

token이 가지고 있는 정수를 vector로 변환해준다. (1d  array에 저장되는..)

cos은 일치하면 1, 반대방향이면 -1로 similarity를 잘 나타낼 수 있다.

**cosine similarity를 사용하자**

![](https://blog.kakaocdn.net/dna/bnVFYc/btsMPwRrgsK/AAAAAAAAAAAAAAAAAAAAAFwMI_RFNhUXeA1wvEibFKq8NWo2SlRevX7Hn-hkuv3r/img.jpg?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1772290799&allow_ip=&allow_referer=&signature=L9A0A7s65x0Tr3j2%2FFT22bc1tr0%3D)

이렇게 얻은 vector들을 similarity에 맞게 학습시키면 된다.

**정리하면**

1. tokenization (단어 쪼개기)

2. encoding (정수 변환)

3. embedding lookup (벡터 변)

## **RNN (Recurrent Neural Network)**

**sequential data를 학습하기 위한 모델이다.**

시간, 순서를 나타내는 축이 존재하는 데이터 (주식, 텍스트, 비디오 등등)

순서를 고려하면, params 수 줄이고, 학습 효율을 올릴 수 있다.

하지만

**MLP는 데이터의 순서를 따로 고려하지 않는다.**

순서를 고려하지 않는다 == inductive bias를 고려하지 않는다,

**CNN도 데이터 순서를 고려하지 않는다.**

scanning으로 전체적인 순서는 고려한다고 볼 수 있지만, kernel 안에서는 고려하지 않는다.

**Inductive bias**

이전 시점의 입력들 x1, x2, .... xt-1과 xt 관계를 모델링할 수 있다.

그 관계는 t와 상관없이 적용 가능하다.

첫 번째 model

### **Elman Network (Simple RNN)**

![](https://blog.kakaocdn.net/dna/b10aCc/btsMNSaSL2V/AAAAAAAAAAAAAAAAAAAAAJin_TQVt1xdOosDls4Ag2eH4nQI2TwXPbEePJSxvUrV/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1772290799&allow_ip=&allow_referer=&signature=heQBik1%2FWbHFKq%2Bm%2FmCJpAjYho0%3D)

Wi 와 Wh가 고정이다.

**forward pass:** Wi: Wh 비율로 sigmoid activation을 계속 통과하는데, output에 도달했을 때는 x1은 영향을 거의 주지 못하게 된다.

(long - term memory 문제)

**backward pass:**vanishing gradient

### 

### **LSTM(Long Short Term Memory)**

xt​: 현재 시점의 입력 데이터

ht​: hidden state

Ct: cell state

ft : it = 이전 데이터의 비율 : 현재 데이터의 비

![](https://blog.kakaocdn.net/dna/c8SsSO/btsMO061H3I/AAAAAAAAAAAAAAAAAAAAAG0RgB_BM-q22ix7IKm20IPWG6-aQoOi26zmpIpns1gi/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1772290799&allow_ip=&allow_referer=&signature=54Sh5NdRt0jgjFMttsOsG515S7k%3D)

forget gate:

![](https://blog.kakaocdn.net/dna/bNlyYY/btsMPpyAmz1/AAAAAAAAAAAAAAAAAAAAAAbxSXTbk1NfIRG-Nt9VlUIoz8UjUVy2nBB140yYI8Qw/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1772290799&allow_ip=&allow_referer=&signature=baIN%2FRktJXUBK36GEa2%2F7dKgRbQ%3D)

input gate:

![](https://blog.kakaocdn.net/dna/qyH5o/btsMOCyypro/AAAAAAAAAAAAAAAAAAAAADZ4Tp4GdJrV0BAoQUw3m2xrbb2ZRKyqLX2WXyGacU1v/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1772290799&allow_ip=&allow_referer=&signature=L4LLz4pJn4E3jm4y1jQS%2FU0paqs%3D)

output gate:

![](https://blog.kakaocdn.net/dna/lMReo/btsMODYuJQ6/AAAAAAAAAAAAAAAAAAAAAFl9n-bu74um_p93FWD4aCTvJA5XrxRhNGQsuSgoV7t1/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1772290799&allow_ip=&allow_referer=&signature=RyuRqocYPy49xmW4KvjnTODK1ag%3D)
![](https://blog.kakaocdn.net/dna/PRug0/btsMOfjsp0n/AAAAAAAAAAAAAAAAAAAAAMOy8skWxCJ7G8_zUPPaferJ45dkwHwvi9y04T0w2okR/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1772290799&allow_ip=&allow_referer=&signature=I%2B4NaYBzURQve9wBRBK6yVZqUbk%3D)

**elman networkd와 의 차이점은**

elman은 이전 값의 비율과 현재 값의 비율이 고정이었지만, (Wi, Wh)

**LSTM에서는 Ft, It은 변경될 수 있다.**

**=>long term memory 보완**

**파라매터 수: Simle RNN \* 4**

### **GRU (Gated Recurrent Unit)**

![](https://blog.kakaocdn.net/dna/cjdYKC/btsMNLW9YE0/AAAAAAAAAAAAAAAAAAAAAA3pUl_41W8_DZhAB6Tw-Dvni07yJNpf9_uvt9x0y8Oe/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1772290799&allow_ip=&allow_referer=&signature=Dal7HuXNDJpF2DyaRRvO0lgH5TE%3D)

LSTM보다 **더 적은 수의 게이트**를 사용하여 계산량을 줄이고, 빠르게 학습할 수 있도록 설계되었다.

Zt: update gate

Rt: reset gate

Ht: hidden state (여기서도 비율을 정하는 모습을 볼 수 있다.)

![](https://blog.kakaocdn.net/dna/oi4Y2/btsMPQCuDDG/AAAAAAAAAAAAAAAAAAAAAHucxH77U7L_w5G9mOV33V7iNLuvdz-u1D6YvzKJ6KdE/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1772290799&allow_ip=&allow_referer=&signature=Bpz9H5rWfr8wPwHwq7alHhUvVv4%3D)

**파라매터 수: Simle RNN \* 3**

### 

### **Bidirectional RNN**

**x1 ~> xt , xt ~> x1 2번 순회하는 RNN이다.**

이것 또한 long term memory / vanishing gradient 를 완화하기 위한 방법이다.

![](https://blog.kakaocdn.net/dna/dzQrFR/btsMOQQ0cxp/AAAAAAAAAAAAAAAAAAAAAGsHMCKuGglQ_dheQZhLGFSOWRcETFWt9BEvWyUHfc6n/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1772290799&allow_ip=&allow_referer=&signature=0BIuSnlCa55n1gxtPjZqk%2FqAajA%3D)

### **Stacked RNN**

forward pass의 time complexity는 여전히 O(L)이다. 동시에 진행되면 되니까

층에 따른 효과 개선이 두드러지지는 않는다.

![](https://blog.kakaocdn.net/dna/ccpMww/btsMPnARH0J/AAAAAAAAAAAAAAAAAAAAAL-g7V0Vb7NQWbvV-8VYCgcGMwt6HVYxiZqLh8Uw_P3t/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1772290799&allow_ip=&allow_referer=&signature=a%2BZQHb3dFTAzLC3k5FEx1D%2FLcYI%3D)

### **Seq2Seq**

**입력과 출력의 길이가 다른 경우 사용되는 model이다.**

주로 **자연어 처리(NLP) 분야**에서 **기계 번역, 텍스트 요약, 챗봇, 음성 인식** 등에 사용된다.

**techer forcing 기법**

![](https://blog.kakaocdn.net/dna/bFbaPB/btsMO6MTGMi/AAAAAAAAAAAAAAAAAAAAABiBGxOcqrb0bFio02hhyhOQMfdZhzShAMZZjCV1mT9H/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1772290799&allow_ip=&allow_referer=&signature=Vgkz7MiG74TSjVkVJ7%2BqMi4BY9c%3D)

**학습:**

입력 sequence와 EOS, 출력 sequence를 이어붙히고, 출력 sequence의 다음 token을 예측하도록 한다.

**추론:**

1. Encoder:

입력 시퀀스 A,B,C를 처리한다.EOS(end of state)전까지의 정보로 Context Vector가 만들어진다.

2. Contex Vector

입력 문장의 전체 정보를 요약하는 벡터

encoder의 마지막 hidden state가 decoder로 전달된다.

3. Decoder

EOS token을 시작으로 한 단어씩 생성

이전 출력을 다음 입력으로 사용

EOS token이 출력되면 종료

###