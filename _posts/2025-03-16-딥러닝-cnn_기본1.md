---
title: "[딥러닝] CNN_기본(1)"
date: 2025-03-16
toc: true
categories:
  - "Tistory"
tags:
  - "tistory"
---

자... 지금 CNN을 배운다고 가정하면, 아직 다차원 tensor를 처리하는 법을 모를 것 이다.

**CNN이란?**

spatial information 에서 semantic informaion으로 변환하는 것

**마지막에 다시 돌아왔을 때 이 말의 뜻이 이해가 되기를...**

1D-tensor만 처리하는 방법을 안다고 가정할 떄

128(높이) \* 128(넓이) \* 3(channle) 데이터를 처리하려고하면 1D로 처리할 것이다.

MLP(or perceptron)으로 처리한다고 생각하면 된다.

첫 번째 FC layer(256일 때)의 파라미터 수는 128 \* 128 \* 3 \* 256 (= 12,582,912)일 것이다. (bias를 사용하지 않는다면)

**데이터 수 보다 파라미터수가 압도적으로 많기 때문에 정보손실이 발생할 확률이 높다. (차원의 저주)**

**inductive bias를 활용할 수 없다**. 이미지는 주변 픽셀끼리의 연관성이 높다. (바나나가 있다고 할 때, 바나나 중앙 pixel의 주변 픽셀들도 바나나일 것이고, 그 주변의 주변의 픽셀도,,, 그 주주주변변변의 픽셀도... )

=> Shift Invariant가 안된다 ( 바나나의 위치가 바뀌면, 같은 바나나가 위치를 옮긴 것으로 생각하지 않고 새로운 패턴으로 본다)

위와 같은 이유로 MLP만으로 이미지를 처리하기 어렵다.

### **Scanning**

**object detection을 예를 들어서 설명하면**

scan을 할 경우 parameter를 재사용하는 효과가 있고, parameter가 input에 영향을 받지 않는다.

=> 이 내용은 아래에서 추가적으로 설명할 예정

3x3(크기는 본인이 설정할 수 있다.) **kernel을 만들고** **kernel을 연산한다( 9칸을 연산하고 ).**

연산하고 **1칸 옆으로 가서 다시 연산...을 반복**하며 **우측에 있는 데이터를 새로 생성**한다.

그리고 **그 데이터에 max를 취해서** 물체가 있었는지 없었는지 판별할 수 있다.

사과가 detection 프로그램이었을 때

만약 사과의 위치가 옮겨지면, 값은 어떻게 변할까?

최대값이 존재하는 위치만 바뀌고 다른 값들은 유지가 될 것이다.  => Shift Invariant한 특성을 가지게 된다.

![](https://blog.kakaocdn.net/dna/ytFbb/btsMK2dn2U2/AAAAAAAAAAAAAAAAAAAAAGwhACyNW9aboFju-zqFfG5ExKY5c-cPfYgkXhfw431a/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1772290799&allow_ip=&allow_referer=&signature=h%2Fw7ejntRFeByZ3k7Iwaq23kBcQ%3D)

그럼 학습 방법은 어떻게 될까?

**나중에 정리..**

마지막 단에 MLP를 통해 최종 출력을 한다.

이런 경우에는 학습을 end to end로 한다.

y**값에 대해 backpropagation해서 마지막 단 weight ~ kernel을 업데이트한다.**

![](https://blog.kakaocdn.net/dna/72pPj/btsMLNGEaV2/AAAAAAAAAAAAAAAAAAAAAPCOnONa6vvK4h05RuW4EqAc0HktqYdXHwb3hcLPcQKV/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1772290799&allow_ip=&allow_referer=&signature=eKlEkF76km8zyHXJPX9dYs2DNPg%3D)

결론적으로

CNN은 MLP에 Scanning을 합친 것으로 볼 수 있다

### 

### **Parameter에 대한 추가 설명**

좌측은 fully connected layer, 우측은 convolution layer이다.

![](https://blog.kakaocdn.net/dna/bbi17e/btsMLXvGxy2/AAAAAAAAAAAAAAAAAAAAANpSmQ4mHiKACo-FVAwYpO9NorfsZuuVUM0B7UwKahAf/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1772290799&allow_ip=&allow_referer=&signature=hTdYpewplMdov6ykFb5W4tb6Zsw%3D)![](https://blog.kakaocdn.net/dna/dGInhT/btsMM4N7MeT/AAAAAAAAAAAAAAAAAAAAAAmq12HjiQT6jawNn-MQv-ETCoZ__qbBp5jT9T_c_T8P/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1772290799&allow_ip=&allow_referer=&signature=9yXf5H17URaAJFiNO55FQb6erT0%3D)

우선 5x5이미지, 3x3 kernel이라고 생각하겠다.

parameter 수를 계산해보자

fully connected: 25  \* 9

convolution: 9

**왜 9야! 저렇게 선분이 많은데!**

convolution은 3 \*3 kernel을 재사용한다. 그렇기 때문에 선분은 많지만 동일한 값들!  
결국 9개의 값만을 사용한다는 것

**OK 그럼 parmameter 업데이트는 어떻게 해?** 그림에서 처럼 w가 z1, z2, z3에서도 사용되는데?

간단하다...! 각각 구한 값(gradient)을 더해주면 된다.

### 

### **Stacked Convolutions ( Convolution을 쌓아보자 )**

Convolution layer를 반복하다보면 feature map의 size가 점점 작아지게 된다.

작아진 feature map을 flatten을 해주면 **차원의 저주 문제에서 해방될 수 있다~**

그러나 **feature map이 줄어들면서 정보의 손실이 일어난다.**

어떻게 방지하냐..

![](https://blog.kakaocdn.net/dna/n496N/btsMKUmd16E/AAAAAAAAAAAAAAAAAAAAAJjG1-offRzB9sBgxurDHFMw_HwtEAr-vKd9X1JRC3oz/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1772290799&allow_ip=&allow_referer=&signature=NdX4pBdyXMsOE%2FUfuhnQrxLHEVc%3D)

값이 다른 parameter ( kernel) 를 여러 개 만들어서 정보 손실을 줄이면 된다.

![](https://blog.kakaocdn.net/dna/Bnong/btsMMeDLrRo/AAAAAAAAAAAAAAAAAAAAACj4jyxF7jRKnoZBZ4jMIdSygoNEQq5ERNsrhqNJ6bPA/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1772290799&allow_ip=&allow_referer=&signature=siRrNX4kJTZ0t8NvnSQSUpXsIP0%3D)

**이제 다시 paramter 수를 세보자**

첫 번째 layer의 paramter수는

우리가 만들어준kernerl의 수는 4개, 커널의 크기는 3 \* 3 \* 3

=> 4 \* ( 3 \* 3 \* 3 ) 으로 볼 수 있다. ( bias를 사용하지 않는다면, bias는 수는 kernel의 수 (4)이다.)

### **Padding**

**kernel - 1 만큼 feature map의 크기가 줄어든다.**

하지만 output feature map의 size를 처음 input size와 동일하게 유지할 때가 유리할 경우가 생긴다.

이때 zero padding을 사용하면된다.

위, 아래에 kernel의 height - 1 만큼 padding을

좌, 우에 kernel의 width - 1만큼 padding을 해주면 input size가 보존이 된다. ( same padding )

![](https://blog.kakaocdn.net/dna/bm4oi7/btsMKTOm0Ur/AAAAAAAAAAAAAAAAAAAAAOOIsaxBTWujJnKcEXNbPhywB1O7FyX8FBWwwjmgaacK/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1772290799&allow_ip=&allow_referer=&signature=jg2vG%2F4a55RQRFkOhxsSqIq1C4Y%3D)

아래는 padding에 따른 output feature map의 크기를 구하는 식이다.

I: input size

F: kernel size

P: padding size

S: stride size

![](https://blog.kakaocdn.net/dna/N6txq/btsMNjdbBBK/AAAAAAAAAAAAAAAAAAAAAJT6VWSM-ONWUVh1tproxFfr2OL81Mi3xyGgg-RG0VTD/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1772290799&allow_ip=&allow_referer=&signature=xbNxe8ooCA88emG%2FdgP8rQIHOzw%3D)

### **Pooling**

stride의 배수 만큼 작아진다.

![](https://blog.kakaocdn.net/dna/cVURgE/btsMLKKaksc/AAAAAAAAAAAAAAAAAAAAAN_CE1KE5eUJv3TuNyjp8CiI9CfSdzGq1DhE_GiYEe5R/img.jpg?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1772290799&allow_ip=&allow_referer=&signature=sfkmxy5j1%2BzLwMR3Tq6Sed%2BTuwY%3D)

### **Unpooling**

stride의 배수만큼 커진다.

![](https://blog.kakaocdn.net/dna/bshBFp/btsMK7TaUSS/AAAAAAAAAAAAAAAAAAAAAIECEFn6we_QLwl8pT3Gpf9no9ad6cLAHhq-UANNvS4W/img.png?credential=yqXZFxpELC7KVnFOS48ylbz2pIh7yKj8&expires=1772290799&allow_ip=&allow_referer=&signature=c5sPv5%2B9r%2BeYhEWCSF5Nxg6zAJo%3D)